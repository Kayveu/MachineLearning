install.packages("swirl")
library("swirl")
swirl()
0
swirl()
needed  <-  c("class")  #class contains the knn() function
installIfAbsentAndLoad(needed)
set.seed(5072)
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
rm(list=ls())
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed  <-  c("class")  #class contains the knn() function
installIfAbsentAndLoad(needed)
set.seed(5072)
x <- matrix(rnorm(20 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 10), rep("Up", 10))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = 1.5, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y)
n <- nrow(mydata)
mydata[c(1:5, (n/2 + 1):(n/2 + 5)),]
set.seed(5072)
x <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 1000), rep("Up", 1000))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = .5, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y, stringsAsFactors = F)
n <- nrow(mydata)
trainprop <- 0.70  #say we want a 70/20/10 split of rows for training, validation and test respectively
validateprop <- 0.2
train  <-  sample(n, trainprop * n)
validate  <-  sample(setdiff(1:n, train), validateprop * n)
test <- setdiff(setdiff(1:n, train), validate)
trainset <- mydata[train,]
validateset <- mydata[validate,]
testset <- mydata[test,]
train.x <- trainset[-3]
train.y <- trainset$y
validate.x <- validateset[-3]
validate.y <- validateset$y
test.x <- testset[-3]
test.y <- testset$y
train.x <- trainset[-3]
train.y <- trainset$y
validate.x <- validateset[-3]
validate.y <- validateset$y
test.x <- testset[-3]
test.y <- testset$y
knn.pred <- knn(train.x, validate.x,  train.y, k=1)
mytable <- table(validate.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(validate.y != knn.pred))
knn.pred <- knn(train.x ,train.x  ,train.y  , k=1)
mytable <- table(train.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(train.y != knn.pred))
numreps <- 100
validate.errors <- rep(0, numreps)
train.errors <- rep(0, numreps)
for(k in 1:numreps) {
knn.pred <- knn(train.x, validate.x,  train.y, k = k)
validate.errors[k] <- mean(validate.y != knn.pred)
knn.pred <- knn(train.x, train.x,  train.y, k = k)
train.errors[k] <- mean(train.y != knn.pred)
}
print(paste("Minimum validate set error rate occurred at k =", which.min(validate.errors)))
print(paste("Minimum validate error rate was ", validate.errors[which.min(validate.errors)]))
typeof(train.errors)
head(train.errors)
head(train.errors)
head(validate.errors)
mytable <- table(validate.y, knn.pred)
numreps <- 100
validate.errors <- rep(0, numreps)
train.errors <- rep(0, numreps)
for(k in 1:numreps) {     #trying to find optimal k
knn.pred <- knn(train.x, validate.x,  train.y, k = k)
mytable <- table(validate.y, knn.pred)
validate.errors[k] <- mean(validate.y != knn.pred)
knn.pred <- knn(train.x, train.x,  train.y, k = k)
train.errors[k] <- mean(train.y != knn.pred)    # a simpler way to compute the error rate
}
print(paste("Minimum validate set error rate occurred at k =", which.min(validate.errors)))
rm(list=ls())
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed  <-  c("class")  #class contains the knn() function
installIfAbsentAndLoad(needed)
set.seed(5072)
x <- matrix(rnorm(20 * 2, 100, 25), ncol = 2)  #matrix of 40 elements with two columns
y <- c(rep("Down", 10), rep("Up", 10))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = 2, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y)
n <- nrow(mydata)
mydata[c(1:5, (n/2 + 1):(n/2 + 5)),]
set.seed(5072)
x <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 1000), rep("Up", 1000))
x[y == "Down",] <- x[y == "Down",] + 25
mydata <- data.frame(x , y, stringsAsFactors = F)
n <- nrow(mydata)
trainprop <- 0.70  #say we want a 70/20/10 split of rows for training, validation and test respectively
validateprop <- 0.2
train  <-  sample(n, trainprop * n)
validate  <-  sample(setdiff(1:n, train), validateprop * n)
test <- setdiff(setdiff(1:n, train), validate)
trainset <- mydata[train,]
validateset <- mydata[validate,]
train.x <- trainset[-3] #do not include third column
testset <- mydata[test,]
validate.x <- validateset[-3]
train.y <- trainset$y
test.x <- testset[-3]
validate.y <- validateset$y
knn.pred <- knn(train.x, validate.x,  train.y, k=1)
test.y <- testset$y
mytable
mytable <- table(validate.y, knn.pred)
knn.pred <- knn(train.x, train.x, train.y, k=1)   #fill in train.x, train.x, train.y to grab training error rate
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)  #error rate, (total up & down + total down & up) divided by total
mytable <- table(train.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
numreps <- 100
validate.errors <- rep(0, numreps)
train.errors <- rep(0, numreps)
for(k in 1:numreps) {     #trying to find optimal k
knn.pred <- knn(train.x, validate.x,  train.y, k = k)
mytable <- table(validate.y, knn.pred)
validate.errors[k] <- mean(validate.y != knn.pred)
knn.pred <- knn(train.x, train.x,  train.y, k = k)
train.errors[k] <- mean(train.y != knn.pred)    # a simpler way to compute the error rate
}
print(paste("Minimum validate set error rate occurred at k =", which.min(validate.errors)))
print(paste("Minimum validate error rate was ", validate.errors[which.min(validate.errors)]))
par(mfrow=c(2,1))
head(train.errors)
plot(numreps:1,validate.errors[order(length(validate.errors):1)],type='n',xlim=c(numreps,1),xlab='Increasing Flexibility (Decreasing k)',ylab='Validation Error Rate',main='Validation Error Rates as a function of \n Flexibility for KNN Prediction')
lines(numreps:1,validate.errors[order(length(validate.errors):1)],type='b', pch=16, col=2)
plot(numreps:1,train.errors[order(length(train.errors):1)],type='n',xlim=c(numreps,1),xlab='Increasing Flexibility (Decreasing k)',ylab='Training Error Rate',main='Training Error Rates as a function of \n Flexibility for KNN Prediction')
lines(numreps:1,train.errors[order(length(train.errors):1)], type='b', pch=16, col=2)
par(mfrow=c(1,1))
rm(list=ls())
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed  <-  c("class")  #class contains the knn() function
installIfAbsentAndLoad(needed)
x <- matrix(rnorm(20 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 10), rep("Up", 10))
x[y == "Down",] <- x[y == "Down",] + 25
mydata <- data.frame(x , y)
n <- nrow(mydata)
mydata[c(1:5, (n/2 + 1):(n/2 + 5)),]
set.seed(5072)
set.seed(5072)
x <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 1000), rep("Up", 1000))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = .5, pch = ifelse(y == "Up", 15, 16))
plot(x, col = ifelse(y == "Up", 4, 2), cex = 1.5, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y, stringsAsFactors = F)
n <- nrow(mydata)
trainprop <- 0.70  #say we want a 70/20/10 split of rows for training, validation and test respectively
validateprop <- 0.2
validate  <-  sample(setdiff(1:n, train), validateprop * n)
trainset <- mydata[train,]
validateset <- mydata[validate,]
testset <- mydata[test,]
train  <-  sample(n, trainprop * n)
train.x <- trainset[-3]
test <- setdiff(setdiff(1:n, train), validate)
train.y <- trainset$y
validate.x <- validateset[-3]
validate.y <- validateset$y
test.x <- testset[-3]
test.y <- testset$y
knn.pred <- knn(train.x, validate.x,  train.y, k=1)
mytable <- table(validate.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(validate.y != knn.pred))        #counts total number of falses divided by n
knn.pred <- knn(train.x ,train.x  ,train.y  , k=1)
mytable <- table(train.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(train.y != knn.pred))
numreps <- 100
validate.errors <- rep(0, numreps)
train.errors <- rep(0, numreps)
for(k in 1:numreps) {
knn.pred <- knn(train.x, validate.x,  train.y, k = k)
validate.errors[k] <- mean(validate.y != knn.pred)    #improving errors as k increases to a certain extent
knn.pred <- knn(train.x, train.x,  train.y, k = k)
train.errors[k] <- mean(train.y != knn.pred)          #increasing errors as k increases for the training set
}
print(paste("Minimum validate set error rate occurred at k =", which.min(validate.errors)))
print(paste("Minimum validate error rate was ", validate.errors[which.min(validate.errors)]))
plot(NULL, NULL, type='n', xlim=c(numreps, 1), ylim=c(0,max(c(validate.errors, train.errors))), xlab='Increasing Flexibility (Decreasing k)', ylab='Error Rates', main='Error Rates as a Function of \n Flexibility for KNN Classification')
lines(seq(numreps, 1), validate.errors[length(validate.errors):1], type='b', col=2, pch=16)
lines(seq(numreps, 1), train.errors[length(train.errors):1], type='b', col=1, pch=16)
legend("topleft", legend = c("Validation Error Rate", "Training Error Rate"), col=c(2, 1), cex=.75, pch=16)
rm(list=ls())
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed  <-  c("class")  #class contains the knn() function
installIfAbsentAndLoad(needed)
set.seed(5072)
x <- matrix(rnorm(20 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 10), rep("Up", 10))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = 1.5, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y)
n <- nrow(mydata)
mydata[c(1:5, (n/2 + 1):(n/2 + 5)),]
set.seed(5072)
x <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
y <- c(rep("Down", 1000), rep("Up", 1000))
x[y == "Down",] <- x[y == "Down",] + 25
plot(x, col = ifelse(y == "Up", 4, 2), cex = .5, pch = ifelse(y == "Up", 15, 16))
mydata <- data.frame(x , y, stringsAsFactors = F)
n <- nrow(mydata)
trainprop <- 0.70  #say we want a 70/20/10 split of rows for training, validation and test respectively
validateprop <- 0.2
train  <-  sample(n, trainprop * n)
validate  <-  sample(setdiff(1:n, train), validateprop * n)
test <- setdiff(setdiff(1:n, train), validate)
trainset <- mydata[train,]
validateset <- mydata[validate,]
testset <- mydata[test,]
train.x <- trainset[-3]
train.y <- trainset$y
validate.x <- validateset[-3]
validate.y <- validateset$y
test.x <- testset[-3]
test.y <- testset$y
knn.pred <- knn(train.x, validate.x,  train.y, k=1)
mytable <- table(validate.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(validate.y != knn.pred))        #counts total number of falses divided by n
knn.pred <- knn(train.x ,train.x  ,train.y  , k=1)
mytable <- table(train.y, knn.pred)
mytable
(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)
(mean(train.y != knn.pred))
numreps <- 100
validate.errors <- rep(0, numreps)
train.errors <- rep(0, numreps)
for(k in 1:numreps) {
knn.pred <- knn(train.x, validate.x,  train.y, k = k)
validate.errors[k] <- mean(validate.y != knn.pred)    #improving errors as k increases to a certain extent
knn.pred <- knn(train.x, train.x,  train.y, k = k)
train.errors[k] <- mean(train.y != knn.pred)          #increasing errors as k increases for the training set
}
print(paste("Minimum validate set error rate occurred at k =", which.min(validate.errors)))
print(paste("Minimum validate error rate was ", validate.errors[which.min(validate.errors)]))
plot(NULL, NULL, type='n', xlim=c(numreps, 1), ylim=c(0,max(c(validate.errors, train.errors))), xlab='Increasing Flexibility (Decreasing k)', ylab='Error Rates', main='Error Rates as a Function of \n Flexibility for KNN Classification')
lines(seq(numreps, 1), validate.errors[length(validate.errors):1], type='b', col=2, pch=16)
lines(seq(numreps, 1), train.errors[length(train.errors):1], type='b', col=1, pch=16)
legend("topleft", legend = c("Validation Error Rate", "Training Error Rate"), col=c(2, 1), cex=.75, pch=16)
knn.pred <- knn(train.x, test.x,  train.y, k = which.min(validate.errors))
mytable <- table(test.y, knn.pred)
print(mytable)
print(paste("Test set error rate was ",(mytable["Up", "Down"] + mytable["Down", "Up"]) / sum(mytable)))
test.error.rates <- rep(0,100)
for(i in 1:100) {
xi <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
xi[y == "Down",] <- xi[y == "Down",] + 25
knn.pred <- knn(train.x, xi, train.y, k = which.min(validate.errors))
test.error.rates[i] <- mean(y != knn.pred)
}
print(paste("Expected test error rate: ", mean(test.error.rates), "Expected Standard Deviation of test error rate:", sd(test.error.rates)))
typeof(knn.pred)
test.error.rates <- rep(0,1000)
for(i in 1:1000) {
xi <- matrix(rnorm(2000 * 2, 100, 25), ncol = 2)
xi[y == "Down",] <- xi[y == "Down",] + 25
knn.pred <- knn(train.x, xi, train.y, k = which.min(validate.errors))
test.error.rates[i] <- mean(y != knn.pred)
}
print(paste("Expected test error rate: ", mean(test.error.rates), "Expected Standard Deviation of test error rate:", sd(test.error.rates)))
print(which.max(test.error.rates))
print(which.min(test.error.rates))
print(test.error.rates[which.max(test.error.rates)])
print(test.error.rates[which.min(test.error.rates)])
plot(NULL, NULL, type='n', xlim=c(1, numreps), ylim=c(0,max(c(validate.errors, train.errors))), xlab='Increasing Flexibility (Decreasing k)', ylab='Error Rates', main='Error Rates as a Function of \n Flexibility for KNN Classification')
lines(seq(numreps, 1), validate.errors[length(validate.errors):1], type='b', col=2, pch=16)
lines(seq(numreps, 1), train.errors[length(train.errors):1], type='b', col=1, pch=16)
legend("topleft", legend = c("Validation Error Rate", "Training Error Rate"), col=c(2, 1), cex=.75, pch=16)
rm(list = ls())
dnorm(x = 90, mean = 124, sd = 20)
dnorm(x = c(10, 20, 30), mean = 124, sd = 20)
library(ggplot2)
curve(dnorm(x, mean = 124, sd = 20))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 2000))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200))
** Binomial distribution **
## Density (mass) at 3 in a binomial distribution with 5 trials with
#  success probability of 0.4
dbinom(x = 3, size = 5, prob = 0.4)
dbinom(x = c(1,2,3), size = 5, prob = 0.4)
barplot(dbinom(x = 0:5, size = 5, prob = 0.4))
barplot(dbinom(x = 0:5, size = 5, prob = 0.4), names.arg = 0:5)
barplot(dbinom(x = 0:5, size = 5, prob = 0.4), names.arg = 0:5, xlab = 'x')
barplot(dbinom(x = 0:5, size = 5, prob = 0.4), names.arg = 0:5, xlab = 'x', ylab = 'y')
barplot(dbinom(x = 0:5, size = 5, prob = 0.4), names.arg = 0:5, xlab = 'x', ylab = 'y', main = 'B(n = 5, p = 0.4)')
coords <- barplot(dbinom(x = 0:5, size = 5, prob = 0.4), names.arg = 0:5, xlab = 'x', ylab = 'y', main = 'B(n = 5, p = 0.4)')
coords
** Normal distribution **
## Probability of having a value *lower* than 90 in a normal distribution
#  with a mean of 124 and sd of 20.
pnorm(q = 90, mean = 124, sd = 20)
** Normal distribution **
## Probability of having a value *lower* than 90 in a normal distribution
#  with a mean of 124 and sd of 20.
pnorm(q = 90, mean = 124, sd = 20, lower.tail = TRUE)
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(n = 0))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(t = 0))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(t = ()))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(t = 1)
)
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(n = 1))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(h = 1))
sequ <- seq(0, 90, 0.1)
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(h = 1))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200); abline(h = 1))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200)); abline(h = 1))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200)); abline(h = 1)
sequ <- seq(0, 90, 0.1)
polygon(x = c(sequ, 90, 0), y = c(dnorm(c(sequ)), col = 'grey'))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200), abline(h = 1))
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200)); abline(h = 1);
** Normal distribution **
## Probability of having a value *lower* than 90 in a normal distribution
#  with a mean of 124 and sd of 20.
pnorm(q = 90, mean = 124, sd = 20)
curve(dnorm(x, mean = 124, sd = 20), xlim = c(0, 200)); abline(h = 1);
sequ <- seq(0, 90, 0.1)
polygon(x = c(sequ, 90, 0), y = c(dnorm(c(sequ)), col = 'grey'))
polygon(x = c(sequ, 90, 0), y = c(dnorm(c(sequ), 124, 20), 0, 0), col = 'grey'))
polygon(x = c(sequ, 90, 0), y = c(dnorm(c(sequ), 124, 20), 0, 0), col = 'grey')
?require
rm(list = ls())
require(ISLR)
require(MASS)
?MASS
?Auto
?Weekly
?power.t.test
power.t.test(power = .95, sig.level = .05, sd = 8.9, n = 25)
?power.prop.test
power.prop.test(power = .95, p1 = .3, p2 = .26, sig.level = .05, power = .95, alternative = 'one.sided')
power.prop.test(p1 = .260, p2 = .3, sig.level = .05, power = .95, alternative = 'one.sided')
power.prop.test(n = 10, p1 = .260, p2 = .3, sig.level = NULL, power = .95, alternative = 'one.sided')
par(mfrow=c(1,3))
set.seed(5072)
rm(list=ls())
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed  <-  c('MASS', 'leaps')
installIfAbsentAndLoad(needed)
par(mfrow=c(1,3))
set.seed(5072)
train.prop <- .75
train.indexes <- sample(1:nrow(Boston),size = train.prop*nrow(Boston))
train.set <- Boston[train.indexes,]
rm(list = ls())
require(ISLR)
require(MASS)
set.seed(5072)
head(Boston)
head(Weekly)
qnorm(.9, mean = 200, sd = 45)
qunif(.9, min = 300, max = 700)
n <- 10
d1 <- runif(n, min = 300, max = 700)
d2 <- rnorm(n, mean = 200, sd = 45)
hist(d1 + d2)
d2 <- rnorm(n, mean = 200, sd = 45)
hist(d1 + d2)
hist(d1, d2)
hist(d1 + d2)
d1 <- runif(n, min = 300, max = 700)
d2 <- rnorm(n, mean = 200, sd = 45)
hist(d1 + d2)
setwd('..')
setwd('..')
setwd('Documents/')
setwd('R Work Dir/')
setwd('RAssignments/Machine Learning/Assignment 3/')
list.files()
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
QDAtable
something <- confusion(rmweekly.class, rmweekly$Direction)
QDAtable
auto.log.table
knn1.table
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
source('Assignment3.R')
cub.fit <- glm(y ~ poly(x, 3), data = cv.df)
cv.cub <- cv.glm(cv.df, quad.fit)
print(paste('Cubic LOOCV Errors:', cv.cub$delta[1]))
cub.fit <- glm(y ~ poly(x, 3), data = cv.df)
cv.cub <- cv.glm(cv.df, cub.fit)
print(paste('Cubic LOOCV Errors:', cv.cub$delta[1]))
print(quart.fit)
print(summary(quart.fit))
